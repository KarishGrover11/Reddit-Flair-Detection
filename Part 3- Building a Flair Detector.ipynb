{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3- Building a Flair Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Importing Libraries required for the Flair Detection\n",
    "The following libraries were used similar to the part 1 :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Creating a Reddit and a Subreddit Oject ( Same as Part 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/varishgrover/desktop')#changing the working directory to access the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Loading the .env\n",
    "\n",
    "CLIENT_ID= os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET= os.getenv(\"CLIENT_SECRET\")\n",
    "USER_AGENT= os.getenv(\"USER_AGENT\")\n",
    "USER_NAME= os.getenv(\"USER_NAME\")\n",
    "PASSWORD= os.getenv(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditObject= praw.Reddit(client_id= CLIENT_ID,client_secret=CLIENT_SECRET, user_agent=USER_AGENT,\n",
    "user_name=USER_NAME,password=PASSWORD)\n",
    "#PRAW library is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "subredditObject= redditObject.subreddit(\"india\")  #scrapping data from subreddit --> r/india\n",
    "newSubredditObject=subredditObject.new(limit=10)\n",
    "#Scrapping 60 latest entries from the \"new\" section in subreddit r/india. This limit was chosen self chosen\n",
    "#and can be modified easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - The Classifier\n",
    "Here a link to the Reddit post is taken as an input and the output of the following code is the Flair of the particular post. Thus it works like a classifier.\n",
    "\n",
    "In this a submission object is created who's parent class is the reddit class. Using this submission object and the input LINK ,  the particular submission i.e. the post is found in the subreddit r/india and from there the flair name is scrapped . The scrapped flair name is then printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/india/comments/g311nq/the_us_surgeon_general_once_warned_against/\n",
      "Coronavirus\n"
     ]
    }
   ],
   "source": [
    "#CLASSIFIER CODE\n",
    "LINK=input() # Asking the user for an input link\n",
    "SUBMISSION=redditObject.submission(url=LINK)# Creating a submission object\n",
    "print(SUBMISSION.link_flair_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Validification\n",
    "The following codes acts as a Testing Kit. The data that was scrapped in the part 1 are provided one by one to the code in a loop.Firstly the code prints the link of the post/submission and then that link is fed to the submission object. The submission object whihc is created at every iteration of the loop is then used to scrape the Flair name off the submission object. This scraped flair name is now compared to the flair name present in the subreddit object . If both the names match, then \"True\" is printed in the code else it is a \"False\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://redd.it/g318gs/r/india/comments/g318gs/why_is_the_modified_media_continuously_playing/\n",
      "Politics\n",
      "True\n",
      "https://redd.it/g316f3/r/india/comments/g316f3/why_crowd_gathered_only_at_bandra_shiv_sena_vows/\n",
      "Coronavirus\n",
      "True\n",
      "https://redd.it/g316d9/r/india/comments/g316d9/as_the_mercury_soars_heres_one_more_pibfactcheck/\n",
      "Coronavirus\n",
      "True\n",
      "https://redd.it/g315w3/r/india/comments/g315w3/who_directorgenerals_opening_remarks_at_the_media/\n",
      "Coronavirus\n",
      "True\n",
      "https://redd.it/g3131y/r/india/comments/g3131y/so_excited_for_mango_season_oc/\n",
      "Photography\n",
      "True\n",
      "https://redd.it/g311nq/r/india/comments/g311nq/the_us_surgeon_general_once_warned_against/\n",
      "Coronavirus\n",
      "True\n",
      "https://redd.it/g30yu7/r/india/comments/g30yu7/lpg_delivery_men_go_the_extra_mile_to_keep/\n",
      "Non-Political\n",
      "True\n",
      "https://redd.it/g30xdr/r/india/comments/g30xdr/engineering_management_colleges_cant_force/\n",
      "Non-Political\n",
      "True\n",
      "https://redd.it/g30w2w/r/india/comments/g30w2w/to_india_our_lives_are_worth_less_than_1/\n",
      "Coronavirus\n",
      "True\n",
      "https://redd.it/g30v3x/r/india/comments/g30v3x/request_long_shot_in_the_dark_but_i_am_looking/\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The link to a submission or a post is basically the shortlink concatenated to the permalink\"\"\"\n",
    "for i in newSubredditObject:\n",
    "    print(i.shortlink+i.permalink)\n",
    "    SUBMISSION2=redditObject.submission(url=(i.shortlink+i.permalink)) \n",
    "    a=SUBMISSION2.link_flair_text\n",
    "    print(a)\n",
    "    if(i.link_flair_text==a):\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the Classifier:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Classifier created above works perfectly for any submission / post from r/india i.e. one the specified subreddit. If we have to classify the flairs for posts outside this subreddit , we will have to make a small change in the code written.\n",
    "\n",
    "2. As seen from the testing mechanism above, the code has 100 percent success rate in detecting the flairs.\n",
    "\n",
    "3. The classifier might face the time issue . Sometimes it can take more time than usual .\n",
    "\n",
    "4. The classifier used can be used to find the flairs for any submission in the given subreddit. No matter how old it is , we just need the link and the classifier detects the flair easily.\n",
    "\n",
    "5. It is hardly a 3 line code, and hence works efficiently .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
